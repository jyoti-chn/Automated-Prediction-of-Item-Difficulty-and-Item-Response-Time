{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyoti-chn/Automated-Prediction-of-Item-Difficulty-and-Item-Response-Time/blob/main/Response_Time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfWKP4AotWAU",
        "outputId": "586313f3-ab3f-4aa1-8eab-9bc2706aa246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# drive.mount('/content/gdrive/', force_remount=True)\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = '/content/gdrive/MyDrive/ML-Project/train_final.csv'\n",
        "test_file = '/content/gdrive/MyDrive/ML-Project/test_final.csv'\n",
        "gold_file = '/content/gdrive/MyDrive/ML-Project/gold_final.csv'"
      ],
      "metadata": {
        "id": "39ZPoNsFtZKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_final_df = pd.read_csv(train_file)\n",
        "\n",
        "\n",
        "print(train_final_df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bHjWY43talB",
        "outputId": "8b62296b-a9c3-4405-e8d9-c8c4d0885a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          ItemNum  Difficulty  Response_Time\n",
            "count  466.000000  466.000000     466.000000\n",
            "mean   330.444206    0.482532      85.555343\n",
            "std    191.249064    0.307830      28.588745\n",
            "min      2.000000    0.020000      26.960000\n",
            "25%    170.250000    0.240000      64.727500\n",
            "50%    324.500000    0.420000      80.490000\n",
            "75%    494.750000    0.700000     101.642500\n",
            "max    666.000000    1.380000     229.430000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers pandas torch textstat nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2LFQp7stcFn",
        "outputId": "b9552db4-98b4-434a-96fd-69b0a905264c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: textstat in /usr/local/lib/python3.10/dist-packages (0.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.10/dist-packages (from textstat) (0.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "YQlYkP3_tfHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import textstat\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v55Xw4atghQ",
        "outputId": "225c3fff-effd-4bef-e9eb-357f5c877a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # text readability scores (Flesch Reading Ease)\n",
        "    readability_score = textstat.flesch_reading_ease(text)\n",
        "    return readability_score\n",
        "\n",
        "train_final_df['readability_score'] = train_final_df['ItemStem_Text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "9-urXwW0th5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# def encode_data(tokenizer, questions, answers_text, correct_answers, max_length=512):\n",
        "#     input_ids_list = []\n",
        "#     attention_masks_list = []\n",
        "#     for question, answers, correct_answer in zip(questions, answers_text, correct_answers):\n",
        "#         input_ids = []\n",
        "#         attention_masks = []\n",
        "#         # Encode question text\n",
        "#         encoded_dict = tokenizer.encode_plus(question, add_special_tokens=True, max_length=max_length,\n",
        "#                                              pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n",
        "#         input_ids.append(encoded_dict['input_ids'])\n",
        "#         attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "#         # Encode concatenated answer options text\n",
        "#         encoded_dict = tokenizer.encode_plus(answers, add_special_tokens=True, max_length=max_length,\n",
        "#                                              pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n",
        "#         input_ids.append(encoded_dict['input_ids'])\n",
        "#         attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "#         # Encode correct answer text\n",
        "#         encoded_dict = tokenizer.encode_plus(correct_answer, add_special_tokens=True, max_length=max_length,\n",
        "#                                              pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n",
        "#         input_ids.append(encoded_dict['input_ids'])\n",
        "#         attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "#         input_ids_list.append(torch.cat(input_ids, dim=0))\n",
        "#         attention_masks_list.append(torch.cat(attention_masks, dim=0))\n",
        "\n",
        "#     return torch.stack(input_ids_list, dim=0), torch.stack(attention_masks_list, dim=0)\n",
        "\n",
        "def encode_data(tokenizer, texts, max_length=512):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for text in texts:\n",
        "        encoded_dict = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_length,\n",
        "                                              pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
        "\n",
        "# Replace NaN values with empty strings in answer columns to avoid NaN in combined text\n",
        "answer_columns = ['Answer__A', 'Answer__B', 'Answer__C', 'Answer__D', 'Answer__E',\n",
        "                  'Answer__F', 'Answer__G', 'Answer__H', 'Answer__I', 'Answer__J']\n",
        "train_final_df[answer_columns] = ' ' + train_final_df[answer_columns].fillna('')\n",
        "\n",
        "# Combine itemtext and answer columns into 'combined_text' column\n",
        "train_final_df['combined_answer_text'] = train_final_df[answer_columns].agg(' '.join, axis=1)\n",
        "train_final_df['combined_answer_text'] = train_final_df['combined_answer_text'].astype(str)\n",
        "# print(train_final_df.loc[0, 'combined_text'])\n",
        "\n",
        "train_final_df['Answer_Text'] = train_final_df['Answer_Text'].fillna('')\n",
        "\n",
        "# Convert 'type' column to numerical values\n",
        "type_mapping = {'Text': 0, 'PIX': 1}\n",
        "train_final_df['type_numeric'] = train_final_df['ItemType'].replace(type_mapping).astype(int)\n",
        "\n",
        "# Convert 'exam' column to numerical values\n",
        "exam_mapping = {'STEP 1': 0, 'STEP 2': 1, 'STEP 3': 2}\n",
        "train_final_df['exam_numeric'] = train_final_df['EXAM'].replace(exam_mapping).astype(int)\n",
        "\n",
        "# print(train_final_df.loc[422, 'exam_numeric'])\n",
        "# print(f\"Number of NaN values in 'Age' column: {train_final_df['Answer_Text'].isna().sum()}\")\n",
        "\n",
        "input_ids_ques, attention_masks_ques = encode_data(tokenizer, train_final_df['ItemStem_Text'].values)\n",
        "input_ids_answers, attention_masks_answers = encode_data(tokenizer, train_final_df['combined_answer_text'].values)\n",
        "input_ids_correct_ans, attention_masks_correct_ans = encode_data(tokenizer, train_final_df['Answer_Text'].values)\n",
        "# print(input_ids_list.size())\n",
        "readability_scores = torch.tensor(train_final_df['readability_score'].values).unsqueeze(1)\n",
        "ques_type = torch.tensor(train_final_df['type_numeric'].values).unsqueeze(1)\n",
        "exam_type = torch.tensor(train_final_df['exam_numeric'].values).unsqueeze(1)\n",
        "response_time = torch.tensor(train_final_df['Response_Time'].values).unsqueeze(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXdh5CrNtj9A",
        "outputId": "98a33620-7e9b-4765-a932-baf8566fe27c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class QuestionRespnseTimeModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QuestionRespnseTimeModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.regression = nn.Sequential(\n",
        "            nn.Linear(768 + 768 + 768 + 1 + 1 + 1, 128),  # Combine BERT output and readability_score\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),  # Existing dropout layer\n",
        "            nn.Linear(128, 64),  # Additional layer for more complexity\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),  # Additional dropout layer for regularization\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids_ques, input_ids_answers, input_ids_correct_ans, attention_masks_ques, attention_masks_answers, attention_masks_correct_ans, readability_scores, exam_type, ques_type):\n",
        "        bert_output_ques = self.bert(input_ids=input_ids_ques, attention_mask=attention_masks_ques)\n",
        "        pooled_output_ques = bert_output_ques.pooler_output\n",
        "\n",
        "        bert_output_answers = self.bert(input_ids=input_ids_answers, attention_mask=attention_masks_ques)\n",
        "        pooled_output_answers = bert_output_answers.pooler_output\n",
        "\n",
        "        bert_output_correct_ans = self.bert(input_ids=input_ids_correct_ans, attention_mask=attention_masks_correct_ans)\n",
        "        pooled_output_correct_ans = bert_output_correct_ans.pooler_output\n",
        "\n",
        "        combined_features = torch.cat((pooled_output_ques, pooled_output_answers, pooled_output_correct_ans, readability_scores, exam_type, ques_type), dim=1)\n",
        "        return self.regression(combined_features)"
      ],
      "metadata": {
        "id": "vMa0Yc5ZtlwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "dataset_size = len(train_final_df)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "print(\"Sizes:\")\n",
        "print(input_ids_ques.size())\n",
        "print(attention_masks_correct_ans.size())\n",
        "print(readability_scores.size())\n",
        "print(exam_type.size())\n",
        "print(ques_type.size())\n",
        "print(response_time.size())\n",
        "\n",
        "# full_dataset = TensorDataset(input_ids_list[0], input_ids_list[1], input_ids_list[2], attention_masks_list[0], attention_masks_list[1], attention_masks_list[2], readability_scores, exam_type, ques_type, difficulties)\n",
        "full_dataset = TensorDataset(input_ids_ques, input_ids_answers, input_ids_correct_ans, attention_masks_ques, attention_masks_answers, attention_masks_correct_ans, readability_scores, exam_type, ques_type, response_time)\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 4\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3QTvNPdtnkP",
        "outputId": "3bfc1d2a-5077-4e15-823a-880ecbc92fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sizes:\n",
            "torch.Size([466, 512])\n",
            "torch.Size([466, 512])\n",
            "torch.Size([466, 1])\n",
            "torch.Size([466, 1])\n",
            "torch.Size([466, 1])\n",
            "torch.Size([466, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = QuestionRespnseTimeModel().to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EModM39tpX2",
        "outputId": "405a1ad2-bf7e-414d-d897-f44640e9b558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "best_val_loss = float('inf')\n",
        "patience = 7\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    print(f\"Epoch {epoch} :\")\n",
        "    for batch in train_dataloader:\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        input_ques, input_answers, input_correct_ans, mask_ques, mask_answers, mask_correct_ans, readability, exam, ques, responseTime = batch\n",
        "        input_ques = input_ques.to(device).long()\n",
        "        input_answers = input_answers.to(device).long()\n",
        "        input_correct_ans = input_correct_ans.to(device).long()\n",
        "        mask_ques = mask_ques.to(device).long()\n",
        "        mask_answers = mask_answers.to(device).long()\n",
        "        mask_correct_ans = mask_correct_ans.to(device).long()\n",
        "        # inputs = list(zip(input_ques, input_answers, input_correct_ans)).to(device).long()\n",
        "        # masks = list(zip(input_ques, input_answers, input_correct_ans)).to(device).long()\n",
        "        # inputs, masks, readability, exam_type, ques_types, difficulty = batch\n",
        "        # inputs = inputs.to(device).long()\n",
        "        # masks = masks.to(device).long()\n",
        "\n",
        "        readability = readability.to(device).float()\n",
        "        exam = exam.to(device).float()\n",
        "        ques = ques.to(device).float()\n",
        "        responseTime = responseTime.float()\n",
        "        model.zero_grad()\n",
        "        outputs = model( input_ques, input_answers, input_correct_ans, mask_ques, mask_answers, mask_correct_ans, readability, exam,ques )\n",
        "        loss = criterion(outputs.squeeze(), responseTime)\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            batch = tuple(b.to(device) for b in batch)\n",
        "            # input_ques, input_answers, input_correct_ans, mask_ques, mask_answers, mask_correct_ans, readability, exam_type, ques_types, difficulty = batch\n",
        "            # inputs = list(zip(input_ques, input_answers, input_correct_ans)).to(device).long()\n",
        "            # masks = list(zip(input_ques, input_answers, input_correct_ans)).to(device).long()\n",
        "            input_ques, input_answers, input_correct_ans, mask_ques, mask_answers, mask_correct_ans, readability, exam, ques, responseTime = batch\n",
        "            input_ques = input_ques.to(device).long()\n",
        "            input_answers = input_answers.to(device).long()\n",
        "            input_correct_ans = input_correct_ans.to(device).long()\n",
        "            mask_ques = mask_ques.to(device).long()\n",
        "            mask_answers = mask_answers.to(device).long()\n",
        "            mask_correct_ans = mask_correct_ans.to(device).long()\n",
        "            readability = readability.float()\n",
        "            exam = exam.to(device).float()\n",
        "            ques = ques.to(device).float()\n",
        "            responseTime = responseTime.float()\n",
        "            outputs = model( input_ques, input_answers, input_correct_ans, mask_ques, mask_answers, mask_correct_ans, readability, exam, ques)\n",
        "            loss = criterion(outputs, responseTime)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), 'best_model_response_time.pth')\n",
        "        counter = 0  # Reset the counter if validation loss improves\n",
        "    else:\n",
        "        counter += 1  # Increment the counter if validation loss doesn't improve\n",
        "        if counter >= patience:\n",
        "            print(f'Early stopping! No improvement in validation loss for {patience} epochs.')\n",
        "            break  # Exit the training loop\n",
        "    print(f\"Epoch {epoch+1}: Avg Val Loss: {avg_val_loss:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceJX04T0trBH",
        "outputId": "bb2bd613-1a1f-4b1e-f53d-3ec7e6616f36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4, 1])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Avg Val Loss: 6626.99\n",
            "Epoch 1 :\n",
            "Epoch 2: Avg Val Loss: 5464.82\n",
            "Epoch 2 :\n",
            "Epoch 3: Avg Val Loss: 3912.21\n",
            "Epoch 3 :\n",
            "Epoch 4: Avg Val Loss: 2416.81\n",
            "Epoch 4 :\n",
            "Epoch 5: Avg Val Loss: 1326.66\n",
            "Epoch 5 :\n",
            "Epoch 6: Avg Val Loss: 749.24\n",
            "Epoch 6 :\n",
            "Epoch 7: Avg Val Loss: 575.27\n",
            "Epoch 7 :\n",
            "Epoch 8: Avg Val Loss: 556.42\n",
            "Epoch 8 :\n",
            "Epoch 9: Avg Val Loss: 567.59\n",
            "Epoch 9 :\n",
            "Epoch 10: Avg Val Loss: 572.28\n",
            "Epoch 10 :\n",
            "Early stopping! No improvement in validation loss for 3 epochs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scp -r /content/best_model_response_time.pth '/content/gdrive/My Drive/ML-Project/'"
      ],
      "metadata": {
        "id": "2gVkiGQ3tsy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming test_df is your test dataset loaded and preprocessed similarly\n",
        "test_final_df = pd.read_csv(test_file)\n",
        "# input_ids_test, attention_masks_test = encode_data(tokenizer, test_df['ItemStem_Text'].values)\n",
        "\n",
        "# test_df['readability_score'] = test_df['ItemStem_Text'].apply(preprocess_text)\n",
        "# test_df[answer_columns] = ' ' + test_df[answer_columns].fillna('')\n",
        "\n",
        "# # Combine itemtext and answer columns into 'combined_text' column\n",
        "# test_df['combined_answer_text'] = test_df[answer_columns].agg(' '.join, axis=1)\n",
        "# test_df['combined_answer_text'] = test_df['combined_answer_text'].astype(str)\n",
        "# # print(train_final_df.loc[0, 'combined_text'])\n",
        "\n",
        "# test_df['Answer_Text'] = test_df['Answer_Text'].fillna('')\n",
        "# test_df['type_numeric'] = test_df['ItemType'].replace(type_mapping).astype(int)\n",
        "# test_df['exam_numeric'] = test_df['EXAM'].replace(exam_mapping).astype(int)\n",
        "\n",
        "# input_ids_ques_test, attention_masks_ques_test = encode_data(tokenizer, test_df['ItemStem_Text'].values)\n",
        "# input_ids_answers_test, attention_masks_answers_test = encode_data(tokenizer, test_df['combined_answer_text'].values)\n",
        "# input_ids_correct_ans_test, attention_masks_correct_ans_test = encode_data(tokenizer, test_df['Answer_Text'].values)\n",
        "\n",
        "# readability_scores_test = torch.tensor(test_df['readability_score'].values).unsqueeze(1)\n",
        "# ques_type_test = torch.tensor(test_df['type_numeric'].values).unsqueeze(1)\n",
        "# exam_type_test = torch.tensor(test_df['exam_numeric'].values).unsqueeze(1)\n",
        "\n",
        "\n",
        "\n",
        "# Replace NaN values with empty strings in answer columns to avoid NaN in combined text\n",
        "answer_columns = ['Answer__A', 'Answer__B', 'Answer__C', 'Answer__D', 'Answer__E',\n",
        "                  'Answer__F', 'Answer__G', 'Answer__H', 'Answer__I', 'Answer__J']\n",
        "test_final_df[answer_columns] = ' ' + test_final_df[answer_columns].fillna('')\n",
        "\n",
        "test_final_df['readability_score'] = test_final_df['ItemStem_Text'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# Combine itemtext and answer columns into 'combined_text' column\n",
        "test_final_df['combined_answer_text'] = test_final_df[answer_columns].agg(' '.join, axis=1)\n",
        "test_final_df['combined_answer_text'] = test_final_df['combined_answer_text'].astype(str)\n",
        "# print(test_final_df.loc[0, 'combined_text'])\n",
        "\n",
        "test_final_df['Answer_Text'] = test_final_df['Answer_Text'].fillna('')\n",
        "\n",
        "# Convert 'type' column to numerical values\n",
        "type_mapping = {'Text': 0, 'PIX': 1}\n",
        "test_final_df['type_numeric'] = test_final_df['ItemType'].replace(type_mapping).astype(int)\n",
        "\n",
        "# Convert 'exam' column to numerical values\n",
        "exam_mapping = {'STEP 1': 0, 'STEP 2': 1, 'STEP 3': 2}\n",
        "test_final_df['exam_numeric'] = test_final_df['EXAM'].replace(exam_mapping).astype(int)\n",
        "\n",
        "# print(test_final_df.loc[422, 'exam_numeric'])\n",
        "# print(f\"Number of NaN values in 'Age' column: {test_final_df['Answer_Text'].isna().sum()}\")\n",
        "\n",
        "input_ids_ques_test, attention_masks_ques_test = encode_data(tokenizer, test_final_df['ItemStem_Text'].values)\n",
        "input_ids_answers_test, attention_masks_answers_test = encode_data(tokenizer, test_final_df['combined_answer_text'].values)\n",
        "input_ids_correct_ans_test, attention_masks_correct_ans_test = encode_data(tokenizer, test_final_df['Answer_Text'].values)\n",
        "# print(input_ids_list.size())\n",
        "readability_scores = torch.tensor(test_final_df['readability_score'].values).unsqueeze(1)\n",
        "ques_type = torch.tensor(test_final_df['type_numeric'].values).unsqueeze(1)\n",
        "exam_type = torch.tensor(test_final_df['exam_numeric'].values).unsqueeze(1)\n",
        "# difficulties = torch.tensor(test_final_df['Difficulty'].values).unsqueeze(1)\n",
        "\n",
        "test_dataset = TensorDataset(input_ids_ques_test, input_ids_answers_test, input_ids_correct_ans_test, attention_masks_ques_test, attention_masks_answers_test, attention_masks_correct_ans_test, readability_scores, exam_type, ques_type)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FaNZtYAtueh",
        "outputId": "9905ce28-bbb3-440b-9660-8ab0581b642a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = QuestionRespnseTimeModel().to(device)\n",
        "model.load_state_dict(torch.load('best_model_response_time.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deW5qyIWtyD3",
        "outputId": "5a96fc98-eaef-4ecf-9687-73ac4172d859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuestionRespnseTimeModel(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (regression): Sequential(\n",
              "    (0): Linear(in_features=2307, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.1, inplace=False)\n",
              "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.2, inplace=False)\n",
              "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ques, input_answers, input_correct_ans, mask_ques, mask_answers, mask_correct_ans, readability, exam_type, ques_types = [b.to(device) for b in batch]\n",
        "        input_ques = input_ques.to(device).long()\n",
        "        input_answers = input_answers.to(device).long()\n",
        "        input_correct_ans = input_correct_ans.to(device).long()\n",
        "        mask_ques = mask_ques.to(device).long()\n",
        "        mask_answers = mask_answers.to(device).long()\n",
        "        mask_correct_ans = mask_correct_ans.to(device).long()\n",
        "        readability = readability.float()\n",
        "        exam_type = exam_type.to(device).float()\n",
        "        ques_types = ques_types.to(device).float()\n",
        "        outputs = model( input_ques, input_answers, input_correct_ans, mask_ques, mask_answers, mask_correct_ans, readability, exam_type, ques_types)\n",
        "        predictions.extend(outputs.cpu().numpy())"
      ],
      "metadata": {
        "id": "mv8rYXKQtzjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "# Assuming 'Difficulty' is the column in test_df with true difficulty scores\n",
        "gold_df = pd.read_csv(gold_file)\n",
        "true_response_time =[]\n",
        "itemNums = test_final_df['ItemNum'].values\n",
        "for item in itemNums:\n",
        "    true_response_time.append(gold_df.loc[gold_df['ItemNum'] == item , 'Response_Time'])\n",
        "\n",
        "rmse = sqrt(mean_squared_error(true_response_time, predictions))\n",
        "print(f\"RMSE on Test Data on Response Time: {rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5YTCHazt2sc",
        "outputId": "466dc9b9-6ce7-48a8-a671-b8022110ea6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE on Test Data on Response Time: 25.96172976849861\n"
          ]
        }
      ]
    }
  ]
}